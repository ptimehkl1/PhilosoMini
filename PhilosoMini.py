import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

print("ğŸ§  PhilosoMini: æ¢ç´¢æ™ºèƒ½çš„æœ€å°å®Œæ•´å•å…ƒ\n")
print("=" * 60)


class PhilosoMini(nn.Module):
    """
    æç®€è¯­è¨€æ¨¡å‹ï¼šç”¨æœ€å°‘å‚æ•°å±•ç°è¯­è¨€ç”Ÿæˆçš„æœ¬è´¨

    å“²å­¦æ€è€ƒï¼š
    - æ¯ä¸ªè¯çš„åµŒå…¥å‘é‡ä»£è¡¨å…¶åœ¨"æ„ä¹‰ç©ºé—´"ä¸­çš„åæ ‡
    - çº¿æ€§å±‚å­¦ä¹ è¯ä¸è¯ä¹‹é—´çš„è½¬ç§»æ¦‚ç‡
    - è¿™æ˜¯å¯¹äººç±»è¯­è¨€ç›´è§‰çš„æ•°å­¦æŠ½è±¡
    """

    def __init__(self, vocab_size, embed_dim):
        super(PhilosoMini, self).__init__()

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim

        # è¯åµŒå…¥ï¼šå°†ç¦»æ•£ç¬¦å·æ˜ å°„åˆ°è¿ç»­æ„ä¹‰ç©ºé—´
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # é¢„æµ‹å±‚ï¼šä»å½“å‰è¯çš„æ„ä¹‰é¢„æµ‹ä¸‹ä¸€è¯çš„æ¦‚ç‡åˆ†å¸ƒ
        self.predictor = nn.Linear(embed_dim, vocab_size)

        # è®¡ç®—å‚æ•°æ€»æ•°
        total_params = sum(p.numel() for p in self.parameters())

        print(f"ğŸ”¢ æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"   è¯æ±‡é‡: {vocab_size}")
        print(f"   åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"   åµŒå…¥å±‚å‚æ•°: {vocab_size * embed_dim}")
        print(f"   é¢„æµ‹å±‚å‚æ•°: {embed_dim * vocab_size + vocab_size}")
        print(f"   æ€»å‚æ•°é‡: {total_params}")
        print(f"   (ç›¸å½“äºäººè„‘ç¥ç»è¿æ¥çš„10^-12åˆ†ä¹‹ä¸€)\n")

    def forward(self, input_ids):
        """
        å‰å‘ä¼ æ’­ï¼šæ€ç»´çš„æ•°å­¦åŒ–è¿‡ç¨‹
        input_ids: [batch_size] å½“å‰è¯çš„ID
        è¿”å›: [batch_size, vocab_size] ä¸‹ä¸€è¯çš„æ¦‚ç‡logits
        """
        # æ­¥éª¤1: ç¬¦å·åˆ°æ„ä¹‰çš„è½¬æ¢
        embeddings = self.embedding(input_ids)  # [batch, embed_dim]

        # æ­¥éª¤2: åŸºäºå½“å‰æ„ä¹‰é¢„æµ‹ä¸‹ä¸€è¯
        logits = self.predictor(embeddings)  # [batch, vocab_size]

        return logits

    def get_word_meaning(self, word_id):
        """è·å–è¯æ±‡çš„'æ„ä¹‰å‘é‡'ï¼ˆç”¨äºå“²å­¦åˆ†æï¼‰"""
        with torch.no_grad():
            return self.embedding(torch.tensor([word_id])).squeeze().numpy()

    def analyze_transition_preferences(self, word_id):
        """åˆ†æç»™å®šè¯å¯¹ä¸‹ä¸€è¯çš„åå¥½ï¼ˆå±•ç°å­¦ä¹ åˆ°çš„è¯­è¨€è§„å¾‹ï¼‰"""
        with torch.no_grad():
            logits = self.forward(torch.tensor([word_id]))
            probs = torch.softmax(logits, dim=1).squeeze().numpy()
            return probs


def create_philosophical_dataset():
    """
    åˆ›å»ºä½“ç°è¯­è¨€åŸºæœ¬æ¨¡å¼çš„è®­ç»ƒæ•°æ®

    è®¾è®¡å“²å­¦ï¼šé€‰æ‹©æœ€èƒ½ä½“ç°è¯­è¨€è§„å¾‹çš„è¯æ±‡å’Œå¥å¼
    """

    # ç²¾å¿ƒé€‰æ‹©çš„15ä¸ªè¯æ±‡ï¼šæ¶µç›–ä¸»ä½“ã€åŠ¨ä½œã€å¯¹è±¡ã€ä¿®é¥°
    vocab = {
        '<start>': 0, '<end>': 1,  # è¾¹ç•Œæ ‡è®°
        'å°çŒ«': 2, 'å°ç‹—': 3, 'é¸Ÿå„¿': 4,  # ä¸»ä½“
        'è·‘æ­¥': 5, 'é£ç¿”': 6, 'ç¡è§‰': 7,  # åŠ¨ä½œ
        'å¿«ä¹': 8, 'å®‰é™': 9,  # çŠ¶æ€
        'åœ¨': 10, 'èŠ±å›­': 11, 'å¤©ç©º': 12,  # åœºæ‰€
        'é‡Œ': 13, 'ä¸­': 14  # ä»‹è¯
    }

    id_to_word = {v: k for k, v in vocab.items()}

    # è®­ç»ƒè¯­æ–™ï¼šä½“ç°åŸºæœ¬è¯­è¨€æ¨¡å¼
    training_sentences = [
        "å°çŒ« è·‘æ­¥",
        "å°ç‹— è·‘æ­¥",
        "é¸Ÿå„¿ é£ç¿”",
        "å°çŒ« ç¡è§‰",
        "å°ç‹— ç¡è§‰",
        "å°çŒ« åœ¨ èŠ±å›­ é‡Œ",
        "é¸Ÿå„¿ åœ¨ å¤©ç©º ä¸­",
        "å°çŒ« å¿«ä¹",
        "å°ç‹— å¿«ä¹",
        "é¸Ÿå„¿ å®‰é™"
    ]

    print("ğŸ“– å“²å­¦è®­ç»ƒè¯­æ–™:")
    print(f"   è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    print(f"   è®­ç»ƒå¥å­æ•°: {len(training_sentences)}")
    print("   ç¤ºä¾‹å¥å­:")
    for i, sent in enumerate(training_sentences[:3]):
        print(f"     {i + 1}. {sent}")
    print("   ...(æ›´å¤šå¥å­ä½“ç°åŠ¨ç‰©-åŠ¨ä½œ-åœºæ‰€çš„åŸºæœ¬æ¨¡å¼)\n")

    return vocab, id_to_word, training_sentences


def prepare_training_data(sentences, vocab):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ•°æ®"""

    training_pairs = []

    for sentence in sentences:
        words = ['<start>'] + sentence.split() + ['<end>']
        word_ids = [vocab[word] for word in words]

        # åˆ›å»º(å½“å‰è¯, ä¸‹ä¸€è¯)è®­ç»ƒå¯¹
        for i in range(len(word_ids) - 1):
            current_word = word_ids[i]
            next_word = word_ids[i + 1]
            training_pairs.append((current_word, next_word))

    return training_pairs


def train_model(model, training_pairs, epochs=100):
    """
    è®­ç»ƒè¿‡ç¨‹ï¼šAIå­¦ä¹ è¯­è¨€æ¨¡å¼çš„è¿‡ç¨‹

    å“²å­¦æ„ä¹‰ï¼šé€šè¿‡æœ€å°åŒ–é¢„æµ‹é”™è¯¯ï¼Œæ¨¡å‹é€æ¸å†…åŒ–è¯­è¨€è§„å¾‹
    """

    print("ğŸ“ å¼€å§‹è®­ç»ƒè¿‡ç¨‹:")
    print(f"   è®­ç»ƒè½®æ•°: {epochs}")
    print(f"   å­¦ä¹ ä»»åŠ¡: ç»™å®šå½“å‰è¯ï¼Œé¢„æµ‹æœ€å¯èƒ½çš„ä¸‹ä¸€è¯")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(training_pairs)}\n")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    losses = []

    for epoch in range(epochs):
        total_loss = 0

        for current_word, next_word in training_pairs:
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            current_tensor = torch.tensor([current_word])
            target_tensor = torch.tensor([next_word])

            logits = model(current_tensor)
            loss = criterion(logits, target_tensor)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(training_pairs)
        losses.append(avg_loss)

        if (epoch + 1) % 20 == 0:
            print(f"   ç¬¬{epoch + 1:3d}è½®: å¹³å‡æŸå¤± = {avg_loss:.4f}")

    print("   ğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²å­¦ä¼šåŸºæœ¬çš„è¯­è¨€è½¬ç§»æ¨¡å¼\n")
    return losses


def analyze_learned_patterns(model, vocab, id_to_word):
    """
    åˆ†ææ¨¡å‹å­¦åˆ°çš„è¯­è¨€æ¨¡å¼

    å“²å­¦ä»·å€¼ï¼šè®©æˆ‘ä»¬çœ‹åˆ°AIå¦‚ä½•ç†è§£å’Œç¼–ç è¯­è¨€è§„å¾‹
    """

    print("ğŸ” åˆ†ææ¨¡å‹å­¦åˆ°çš„è¯­è¨€æ¨¡å¼:")

    # åˆ†æå…³é”®è¯çš„è½¬ç§»åå¥½
    key_words = ['<start>', 'å°çŒ«', 'å°ç‹—', 'é¸Ÿå„¿', 'åœ¨']

    for word in key_words:
        if word in vocab:
            word_id = vocab[word]
            probs = model.analyze_transition_preferences(word_id)

            # æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„3ä¸ªåç»­è¯
            top_indices = np.argsort(probs)[-3:][::-1]

            print(f"   '{word}' ä¹‹åæœ€å¯èƒ½çš„è¯:")
            for i, idx in enumerate(top_indices):
                next_word = id_to_word[idx]
                prob = probs[idx]
                print(f"     {i + 1}. '{next_word}' (æ¦‚ç‡: {prob:.3f})")
            print()


def demonstrate_meaning_space(model, vocab, id_to_word):
    """
    æ¼”ç¤ºè¯æ±‡çš„æ„ä¹‰ç©ºé—´

    å“²å­¦å¯å‘ï¼šå±•ç¤ºAIå¦‚ä½•å°†ç¬¦å·è½¬æ¢ä¸ºæ„ä¹‰çš„æ•°å­¦è¡¨ç¤º
    """

    print("ğŸŒŸ æ¢ç´¢è¯æ±‡çš„æ„ä¹‰ç©ºé—´:")

    # è·å–æ‰€æœ‰è¯æ±‡çš„åµŒå…¥å‘é‡
    word_meanings = {}
    for word, word_id in vocab.items():
        if word not in ['<start>', '<end>']:
            meaning_vector = model.get_word_meaning(word_id)
            word_meanings[word] = meaning_vector

    # è®¡ç®—è¯æ±‡é—´çš„ç›¸ä¼¼æ€§
    print("   è¯æ±‡ç›¸ä¼¼æ€§åˆ†æï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰:")

    words = list(word_meanings.keys())
    similarities = []

    for i, word1 in enumerate(words):
        for j, word2 in enumerate(words):
            if i < j:
                vec1 = word_meanings[word1]
                vec2 = word_meanings[word2]

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
                similarities.append((word1, word2, cosine_sim))

    # æ˜¾ç¤ºæœ€ç›¸ä¼¼çš„è¯å¯¹
    similarities.sort(key=lambda x: x[2], reverse=True)
    print("   æœ€ç›¸ä¼¼çš„è¯å¯¹:")
    for word1, word2, sim in similarities[:5]:
        print(f"     '{word1}' â†” '{word2}': {sim:.3f}")

    print()


def generate_text(model, vocab, id_to_word, start_word='<start>',
                  max_length=8, temperature=1.0, show_process=True):
    """
    æ–‡æœ¬ç”Ÿæˆï¼šAIçš„åˆ›é€ è¿‡ç¨‹

    å“²å­¦æ€è€ƒï¼šè¿™æ˜¯ç¡®å®šæ€§æ•°å­¦ä¸éšæœºæ€§åˆ›é€ çš„ç»“åˆ 
    """

    if show_process:
        print(f"ğŸ¨ æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹:")
        print(f"   èµ·å§‹è¯: '{start_word}'")
        print(f"   æœ€å¤§é•¿åº¦: {max_length}")
        print(f"   åˆ›é€ æ€§æ¸©åº¦: {temperature}")
        print()

    model.eval()

    if start_word not in vocab:
        start_word = '<start>'

    current_id = vocab[start_word]
    generated_words = [start_word]

    with torch.no_grad():
        for step in range(max_length - 1):
            # è·å–ä¸‹ä¸€è¯çš„æ¦‚ç‡åˆ†å¸ƒ
            logits = model(torch.tensor([current_id]))

            # åº”ç”¨æ¸©åº¦è°ƒèŠ‚åˆ›é€ æ€§
            if temperature != 1.0:
                logits = logits / temperature

            probs = torch.softmax(logits, dim=1).squeeze()

            # æ¦‚ç‡é‡‡æ ·ï¼ˆä½“ç°åˆ›é€ æ€§ï¼‰
            if temperature > 0:
                next_id = torch.multinomial(probs, 1).item()
            else:
                next_id = torch.argmax(probs).item()

            next_word = id_to_word[next_id]

            if show_process and step < 3:
                top_probs, top_indices = torch.topk(probs, 3)
                print(f"   æ­¥éª¤{step + 1}: é€‰æ‹© '{next_word}' (æ¦‚ç‡: {probs[next_id]:.3f})")
                candidates = [id_to_word[idx.item()] for idx in top_indices]
                candidate_probs = [f"{p:.3f}" for p in top_probs]
                print(f"     å€™é€‰: {candidates}")
                print(f"     æ¦‚ç‡: {candidate_probs}")

            generated_words.append(next_word)

            if next_word == '<end>':
                break

            current_id = next_id

    generated_text = ' '.join([w for w in generated_words if w not in ['<start>', '<end>']])

    if show_process:
        print(f"   âœ¨ ç”Ÿæˆç»“æœ: '{generated_text}'\n")

    return generated_text


def philosophical_reflection(model, vocab):
    """
    å“²å­¦åæ€ï¼šä»æç®€æ¨¡å‹çœ‹æ™ºèƒ½æœ¬è´¨
    """

    print("=" * 60)
    print("ğŸ¤” å“²å­¦åæ€ï¼šæ™ºèƒ½çš„æœ¬è´¨ç‰¹å¾")
    print("=" * 60)

    param_count = sum(p.numel() for p in model.parameters())

    print(f"ğŸ’­ æˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä»€ä¹ˆï¼Ÿ")
    print()
    print(f"1. ã€å‚æ•°çš„å“²å­¦æ„ä¹‰ã€‘")
    print(f"   ä»…ç”¨{param_count}ä¸ªæ•°å­—ï¼Œæ¨¡å‹å°±èƒ½å±•ç°åŸºæœ¬çš„è¯­è¨€èƒ½åŠ›")
    print(f"   è¿™äº›å‚æ•°ç¼–ç äº†è¯æ±‡çš„'æ„ä¹‰'å’Œè¯é—´çš„'å…³ç³»'")
    print(f"   æ„ä¹‰ä¸å†æ˜¯æŠ½è±¡æ¦‚å¿µï¼Œè€Œæ˜¯å…·ä½“çš„æ•°å­¦å‘é‡")
    print()

    print(f"2. ã€å­¦ä¹ çš„æœ¬è´¨ã€‘")
    print(f"   è®­ç»ƒè¿‡ç¨‹å°±æ˜¯ä¸æ–­è°ƒæ•´è¿™äº›æ•°å­—")
    print(f"   æ¯æ¬¡è°ƒæ•´éƒ½è®©æ¨¡å‹æ›´å¥½åœ°é¢„æµ‹'ä¸‹ä¸€ä¸ªè¯'")
    print(f"   è¿™ç±»ä¼¼äºäººç±»é€šè¿‡ç»éªŒä¿®æ­£ç›´è§‰çš„è¿‡ç¨‹")
    print()

    print(f"3. ã€åˆ›é€ æ€§çš„æ•°å­¦åŸºç¡€ã€‘")
    print(f"   æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§ä¸ç¡®å®šæ€§çš„å¹³è¡¡")
    print(f"   åˆ›é€ æ€§å¯èƒ½å°±æ˜¯'æœ‰çº¦æŸçš„éšæœºæ€§'")
    print(f"   AIçš„'æƒ³è±¡åŠ›'æºäºæ¦‚ç‡åˆ†å¸ƒçš„é‡‡æ ·")
    print()

    print(f"4. ã€æ¶Œç°ç°è±¡ã€‘")
    print(f"   ç®€å•çš„æ•°å­¦è¿ç®—é‡å¤åº”ç”¨äº§ç”Ÿäº†å¤æ‚è¡Œä¸º")
    print(f"   æ™ºèƒ½å¯èƒ½æ˜¯å¤æ‚æ€§çš„æ¶Œç°ç‰¹æ€§")
    print(f"   ä»é‡å˜åˆ°è´¨å˜çš„å“²å­¦åŸç†åœ¨æ­¤ä½“ç°")
    print()

    print(f"5. ã€è§„æ¨¡æ•ˆåº”çš„å¯ç¤ºã€‘")
    print(f"   å¦‚æœ{param_count}ä¸ªå‚æ•°èƒ½äº§ç”ŸåŸºæœ¬æ™ºèƒ½...")
    print(f"   é‚£ä¹ˆä¸‡äº¿å‚æ•°çš„æ¨¡å‹ä¼šå±•ç°ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ")
    print(f"   è¿™è®©æˆ‘ä»¬æ€è€ƒæ™ºèƒ½çš„è¾¹ç•Œå’Œå¯èƒ½æ€§")
    print()


def main():
    """
    ä¸»å‡½æ•°ï¼šå®Œæ•´çš„å“²å­¦å®éªŒ
    """

    print("ğŸ”¬ PhilosoMiniå®éªŒï¼šç”¨æœ€å°‘å‚æ•°ç†è§£è¯­è¨€æ™ºèƒ½")
    print("æ¢ç´¢é—®é¢˜ï¼šæ™ºèƒ½çš„æœ€å°å®Œæ•´å•å…ƒæ˜¯ä»€ä¹ˆï¼Ÿ\n")

    # 1. å‡†å¤‡æ•°æ®
    vocab, id_to_word, sentences = create_philosophical_dataset()
    training_pairs = prepare_training_data(sentences, vocab)

    # 2. åˆ›å»ºæ¨¡å‹
    model = PhilosoMini(vocab_size=len(vocab), embed_dim=4)

    # 3. è®­ç»ƒå‰ç”Ÿæˆï¼ˆå±•ç¤ºéšæœºçŠ¶æ€ï¼‰
    print("ğŸ¼ è®­ç»ƒå‰çš„ç”Ÿæˆï¼ˆéšæœºçŠ¶æ€ï¼‰:")
    untrained_text = generate_text(model, vocab, id_to_word,
                                   start_word='å°çŒ«', show_process=False)
    print(f"   æœªè®­ç»ƒè¾“å‡º: '{untrained_text}'")
    print("   ï¼ˆåŸºæœ¬æ˜¯éšæœºçš„ï¼Œæ²¡æœ‰è¯­è¨€é€»è¾‘ï¼‰\n")

    # 4. è®­ç»ƒæ¨¡å‹
    losses = train_model(model, training_pairs, epochs=100)

    # 5. åˆ†æå­¦ä¹ ç»“æœ
    analyze_learned_patterns(model, vocab, id_to_word)
    demonstrate_meaning_space(model, vocab, id_to_word)

    # 6. è®­ç»ƒåç”Ÿæˆæµ‹è¯•
    print("ğŸ§  è®­ç»ƒåçš„æ™ºèƒ½ç”Ÿæˆ:")

    test_starts = ['å°çŒ«', 'é¸Ÿå„¿', 'å°ç‹—']
    temperatures = [0.1, 0.8, 1.5]

    for start_word in test_starts:
        print(f"ğŸ“ ä»¥'{start_word}'å¼€å§‹çš„ç”Ÿæˆ:")
        for temp in temperatures:
            result = generate_text(model, vocab, id_to_word,
                                   start_word=start_word,
                                   temperature=temp,
                                   show_process=False)
            print(f"   æ¸©åº¦{temp}: '{result}'")
        print()

    # 7. å“²å­¦æ€»ç»“
    philosophical_reflection(model, vocab)

    print("âœ¨ å®éªŒç»“è®º:")
    print("   é€šè¿‡è¿™ä¸ªæç®€æ¨¡å‹ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ™ºèƒ½çš„åŸºæœ¬æœºåˆ¶ï¼š")
    print("   - ç¬¦å·åˆ°æ„ä¹‰çš„æ˜ å°„ï¼ˆè¯åµŒå…¥ï¼‰")
    print("   - æ¨¡å¼è¯†åˆ«ä¸é¢„æµ‹ï¼ˆçº¿æ€§å˜æ¢ï¼‰")
    print("   - åˆ›é€ æ€§ç”Ÿæˆï¼ˆæ¦‚ç‡é‡‡æ ·ï¼‰")
    print("   - ä»ç»éªŒåˆ°æ™ºæ…§ï¼ˆå‚æ•°å­¦ä¹ ï¼‰")
    print()
    print("   æ­£å¦‚è€å­æ‰€è¨€ï¼š'é“ç”Ÿä¸€ï¼Œä¸€ç”ŸäºŒï¼ŒäºŒç”Ÿä¸‰ï¼Œä¸‰ç”Ÿä¸‡ç‰©'")
    print("   ç®€å•çš„æ•°å­¦åŸç†ï¼Œç»è¿‡å±‚å±‚ç»„åˆï¼Œæœ€ç»ˆé€šå‘äº†äººå·¥æ™ºèƒ½çš„æ— é™å¯èƒ½ã€‚")

    return model, vocab, id_to_word


# è¿è¡Œå®Œæ•´å®éªŒ
if __name__ == "__main__":
    model, vocab, id_to_word = main()
