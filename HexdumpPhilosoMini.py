import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import struct  # æµ®ç‚¹æ•°åˆ°åå…­è¿›åˆ¶çš„è½¬æ¢


# å®šä¹‰ä¹‹å‰çš„PhilosoMiniæ¨¡å‹å’Œè®­ç»ƒ/æ•°æ®å‡†å¤‡å‡½æ•°
# (ä¸ºäº†ä»£ç çš„å®Œæ•´æ€§ï¼Œè¿™é‡Œå†æ¬¡åŒ…å«å®ƒä»¬ï¼Œå®é™…è¿è¡Œæ—¶å¯ä»¥ä»ä¹‹å‰çš„ä»£ç å¯¼å…¥)

class PhilosoMini(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super(PhilosoMini, self).__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.predictor = nn.Linear(embed_dim, vocab_size)

    def forward(self, input_ids):
        embeddings = self.embedding(input_ids)
        logits = self.predictor(embeddings)
        return logits

    def get_word_meaning(self, word_id):
        with torch.no_grad():
            return self.embedding(torch.tensor([word_id])).squeeze().numpy()

    def analyze_transition_preferences(self, word_id):
        with torch.no_grad():
            logits = self.forward(torch.tensor([word_id]))
            probs = torch.softmax(logits, dim=1).squeeze().numpy()
            return probs


def create_philosophical_dataset():
    vocab = {
        '<start>': 0, '<end>': 1,
        'å°çŒ«': 2, 'å°ç‹—': 3, 'é¸Ÿå„¿': 4,
        'è·‘æ­¥': 5, 'é£ç¿”': 6, 'ç¡è§‰': 7,
        'å¿«ä¹': 8, 'å®‰é™': 9,
        'åœ¨': 10, 'èŠ±å›­': 11, 'å¤©ç©º': 12,
        'é‡Œ': 13, 'ä¸­': 14
    }
    id_to_word = {v: k for k, v in vocab.items()}
    training_sentences = [
        "å°çŒ« è·‘æ­¥", "å°ç‹— è·‘æ­¥", "é¸Ÿå„¿ é£ç¿”",
        "å°çŒ« ç¡è§‰", "å°ç‹— ç¡è§‰",
        "å°çŒ« åœ¨ èŠ±å›­ é‡Œ", "é¸Ÿå„¿ åœ¨ å¤©ç©º ä¸­",
        "å°çŒ« å¿«ä¹", "å°ç‹— å¿«ä¹", "é¸Ÿå„¿ å®‰é™"
    ]
    return vocab, id_to_word, training_sentences


def prepare_training_data(sentences, vocab):
    training_pairs = []
    for sentence in sentences:
        words = ['<start>'] + sentence.split() + ['<end>']
        word_ids = [vocab[word] for word in words]
        for i in range(len(word_ids) - 1):
            current_word = word_ids[i]
            next_word = word_ids[i + 1]
            training_pairs.append((current_word, next_word))
    return training_pairs


def train_model(model, training_pairs, epochs=100):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    for epoch in range(epochs):
        total_loss = 0
        for current_word, next_word in training_pairs:
            optimizer.zero_grad()
            current_tensor = torch.tensor([current_word])
            target_tensor = torch.tensor([next_word])
            logits = model(current_tensor)
            loss = criterion(logits, target_tensor)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
    return model  # è¿”å›è®­ç»ƒåçš„æ¨¡å‹


# è¾…åŠ©å‡½æ•°ï¼šå°†float32è½¬æ¢ä¸ºåå…­è¿›åˆ¶è¡¨ç¤º
def float_to_hex(f):
    # struct.pack('<f', f) å°†float fæ‰“åŒ…æˆå°ç«¯åºçš„4å­—èŠ‚äºŒè¿›åˆ¶æ•°æ®
    # struct.unpack('<I', ...) å°†è¿™4å­—èŠ‚äºŒè¿›åˆ¶æ•°æ®è§£åŒ…æˆæ— ç¬¦å·æ•´æ•°
    # hex(...) å°†æ— ç¬¦å·æ•´æ•°è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
    return hex(struct.unpack('<I', struct.pack('<f', f))[0])


# --- è¿è¡Œè®­ç»ƒå¹¶å±•ç¤ºå‚æ•° ---

print("ğŸŒŸ PhilosoMiniæ¨¡å‹å‚æ•°çš„åå…­è¿›åˆ¶å±•ç¤ºä¸å«ä¹‰è§£æ ğŸŒŸ")
print("=" * 60)

# 1. å‡†å¤‡æ•°æ®
vocab, id_to_word, sentences = create_philosophical_dataset()
training_pairs = prepare_training_data(sentences, vocab)

# 2. åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = PhilosoMini(vocab_size=len(vocab), embed_dim=4)
print("æ¨¡å‹å¼€å§‹è®­ç»ƒ...")
model = train_model(model, training_pairs, epochs=200)  # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œè®©å‚æ•°æ›´ç¨³å®š
print("æ¨¡å‹è®­ç»ƒå®Œæˆï¼\n")

print(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}ä¸ª\n")

# 3. éå†å¹¶å±•ç¤ºæ¯ä¸ªå‚æ•°
for name, param in model.named_parameters():
    print(f"--- å‚æ•°ç»„: {name} --- (å½¢çŠ¶: {param.shape})")

    if "embedding.weight" in name:
        print("ğŸ’¡ å«ä¹‰: è¿™äº›å‚æ•°å®šä¹‰äº†æ¯ä¸ªè¯æ±‡åœ¨'æ„ä¹‰ç©ºé—´'ä¸­çš„ä½ç½®ï¼ˆåæ ‡ï¼‰ã€‚")
        print("   æ¯ä¸ªè¯è¢«4ä¸ªæµ®ç‚¹æ•°ï¼ˆ4ä¸ªç»´åº¦ï¼‰æè¿°ã€‚")
        print("   ä¾‹å¦‚ï¼Œ'å°çŒ«'çš„è¿™4ä¸ªæ•°å­—ï¼Œå°±ä»£è¡¨äº†'å°çŒ«'çš„æŸç§æ•°å­—åŒ–çš„'æ€§æ ¼'æˆ–'å±æ€§'ã€‚\n")

        for i in range(param.shape[0]):  # éå†æ¯ä¸ªè¯
            word = id_to_word[i]
            print(f"   è¯æ±‡ '{word}' (ID:{i}) çš„æ„ä¹‰å‘é‡:")
            for j in range(param.shape[1]):  # éå†æ¯ä¸ªç»´åº¦
                value = param[i, j].item()
                hex_val = float_to_hex(value)
                print(f"     ç»´åº¦ {j + 1}: åè¿›åˆ¶={value:.6f}, åå…­è¿›åˆ¶={hex_val}")
            print()

    elif "predictor.weight" in name:
        print("ğŸ’¡ å«ä¹‰: è¿™äº›å‚æ•°å®šä¹‰äº†è¯æ±‡ä¹‹é—´çš„'è½¬ç§»è§„åˆ™'æˆ–'å¼•åŠ›å¼ºåº¦'ã€‚")
        print("   å®ƒä»¬å†³å®šäº†å½“æ¨¡å‹çœ‹åˆ°ä¸€ä¸ªè¯æ—¶ï¼Œå®ƒçš„'æ„ä¹‰å‘é‡'ä¼šå¦‚ä½•å½±å“ä¸‹ä¸€ä¸ªè¯å‡ºç°çš„å¯èƒ½æ€§ã€‚")
        print(
            "   ä¾‹å¦‚ï¼Œå¦‚æœ'å°çŒ«'çš„'æ´»æ³¼'ç»´åº¦å¾ˆé«˜ï¼Œå¹¶ä¸”'è·‘æ­¥'çš„æŸä¸ªæƒé‡å¯¹'æ´»æ³¼'ç»´åº¦å¾ˆæ•æ„Ÿï¼Œé‚£ä¹ˆ'å°çŒ«'åé¢å°±æ›´å®¹æ˜“å‡ºç°'è·‘æ­¥'ã€‚\n")

        for i in range(param.shape[0]):  # éå†æ¯ä¸ªå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯
            next_word = id_to_word[i]
            print(f"   é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ä¸º '{next_word}' (ID:{i}) çš„æƒé‡ï¼ˆå—å‰ä¸€ä¸ªè¯çš„4ä¸ªç»´åº¦å½±å“ï¼‰:")
            for j in range(param.shape[1]):  # éå†å‰ä¸€ä¸ªè¯çš„æ¯ä¸ªç»´åº¦
                value = param[i, j].item()
                hex_val = float_to_hex(value)
                print(f"     å½±å“æ¥è‡ªå‰ä¸€ä¸ªè¯çš„ç»´åº¦ {j + 1}: åè¿›åˆ¶={value:.6f}, åå…­è¿›åˆ¶={hex_val}")
            print()

    elif "predictor.bias" in name:
        print("ğŸ’¡ å«ä¹‰: è¿™äº›å‚æ•°å®šä¹‰äº†æ¯ä¸ªè¯æ±‡çš„'åŸºç¡€åå¥½'æˆ–'é»˜è®¤å€¾å‘'ã€‚")
        print("   å®ƒä»¬æ˜¯æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•å‰æ–‡ä¿¡æ¯æ—¶ï¼Œå¯¹æŸä¸ªè¯å‡ºç°çš„'åˆå§‹åˆ†æ•°'ã€‚")
        print("   ä¾‹å¦‚ï¼Œå¦‚æœ'è·‘æ­¥'çš„åç½®å¾ˆé«˜ï¼Œé‚£ä¹ˆå®ƒå°±æ¯”åç½®ä½çš„è¯æ›´å®¹æ˜“è¢«é€‰ä½œä¸‹ä¸€ä¸ªè¯ï¼Œå³ä½¿å‰ä¸€ä¸ªè¯å¯¹å®ƒçš„å½±å“ä¸å¤§ã€‚\n")

        for i in range(param.shape[0]):  # éå†æ¯ä¸ªè¯
            word = id_to_word[i]
            value = param[i].item()
            hex_val = float_to_hex(value)
            print(f"   è¯æ±‡ '{word}' (ID:{i}) çš„åŸºç¡€åç½®: åè¿›åˆ¶={value:.6f}, åå…­è¿›åˆ¶={hex_val}")
        print()

print("=" * 60)
print("âœ¨ æ€»ç»“ï¼š")
print("è¿™äº›æ•°å­—ï¼Œæ— è®ºæ˜¯åè¿›åˆ¶è¿˜æ˜¯åå…­è¿›åˆ¶ï¼Œéƒ½æ˜¯PhiloMini'å¤§è„‘'çš„å…¨éƒ¨çŸ¥è¯†ã€‚")
print("å®ƒä»¬å…±åŒå†³å®šäº†æ¨¡å‹å¦‚ä½•ç†è§£è¯æ±‡ã€å¦‚ä½•æ¨ç†è¯æ±‡ä¹‹é—´çš„å…³ç³»ï¼Œæœ€ç»ˆç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚")
print("è™½ç„¶åå…­è¿›åˆ¶çœ‹èµ·æ¥ç¥ç§˜ï¼Œä½†å®ƒåªæ˜¯æµ®ç‚¹æ•°åœ¨è®¡ç®—æœºä¸­çš„ä¸€ç§å­˜å‚¨å½¢å¼ã€‚")
print("çœŸæ­£æœ‰æ„ä¹‰çš„æ˜¯è¿™äº›æµ®ç‚¹æ•°çš„å¤§å°å’Œå®ƒä»¬åœ¨æ¨¡å‹ç»“æ„ä¸­çš„ä½ç½®å’Œä½œç”¨ã€‚")
print("å°±åƒåŸºå› çš„ATCGåºåˆ—æœ¬èº«æ²¡æœ‰æ„ä¹‰ï¼Œä½†å®ƒä»¬ç¼–ç çš„è›‹ç™½è´¨å’Œç”Ÿå‘½åŠŸèƒ½æ‰æœ‰æ„ä¹‰ä¸€æ ·ã€‚")
print("AIçš„æ™ºèƒ½ï¼Œå°±è—åœ¨è¿™äº›çœ‹ä¼¼æ— åºçš„æ•°å­—æ’åˆ—ä¸­ï¼Œé€šè¿‡ç²¾å¦™çš„æ•°å­¦è¿ç®—ï¼Œæ¶Œç°å‡ºç†è§£å’Œåˆ›é€ çš„èƒ½åŠ›ã€‚")
